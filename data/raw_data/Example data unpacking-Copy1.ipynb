{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client(profile='mpi3')\n",
    "dc = rc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dc.push({'cwd':os.getcwd()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import os\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import dill\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pauli_channel_approximation import compute_dpn_and_fid\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\"\"\"\n",
    "These pkl files contain PCA objects that define the Hamiltonian, \n",
    "the controls that were generated, the standard deviation of the controls\n",
    "used in the robust optimization, etc\n",
    "\"\"\"\n",
    "\n",
    "pca228 = dill.load(open(\"pickled_controls228.pkl\", 'rb')) # SQRTY \n",
    "pca245 = dill.load(open(\"pickled_controls245.pkl\", 'rb')) # SQRTX\n",
    "pca261 = dill.load(open(\"pickled_controls261.pkl\", 'rb')) # SQRT(ZZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects have the following attributes:\n",
    "\n",
    " 'ambient_hamiltonian' - the uncontrolled hamiltonians\n",
    "\n",
    "'control_hamiltonians' - the controlled hamiltonians\n",
    "\n",
    "'controlset' - the set of controls generated by grape, indexed i, j with i being the time slice, and j being the control hamiltonian that that amplitude is for.\n",
    "\n",
    "'detunings' - the detunings (stds) for each control, gaussian distributed with 0 mean. \n",
    "\n",
    "'dt' the length of a timestep\n",
    "\n",
    "'num_controls' the number of controls (should equal the length of controlset)\n",
    "\n",
    "'probs' the probabilities that come out of the routine\n",
    "\n",
    "'start' when the algorithm started running\n",
    "\n",
    "'stop' when the algorithm stopped running\n",
    "\n",
    "'target_operator' the unitary we're trying to make\n",
    "\n",
    "'time' the total time the algorithm took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 2\n",
    "num_processors=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "pca = pca245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first plot here varies over each detuning individually, and produces a number of plots equal to the number of controls we have (Like in the paper currently)\n",
    "\n",
    "# Each detuning is .001 in all of these examples (but you can find this in the pca objects) and they correspond to the standard deviations on the controls. (normally distributed with mean 0 and std=detuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in range(len(pca.control_hamiltonians)):\n",
    "    values_to_plot = []\n",
    "    corr = []\n",
    "    for i, detuning in enumerate(pca.detunings):\n",
    "        \n",
    "        # This sets the x axis (It's okay to pick detuning[0] because they were all set to the same detuning.)\n",
    "        values = np.linspace(-detuning[0], detuning[0], num_points)\n",
    "        if i == index:\n",
    "            values_to_plot.append(values)\n",
    "        else:\n",
    "            values_to_plot.append([0])\n",
    "        corr.append(i)\n",
    "        \n",
    "    # This was when I was evaluating more detuning combinations before, \n",
    "    # now this is just indexing over each detuning param.\n",
    "    combinations = itertools.product(*values_to_plot)\n",
    "    new_combinations = []\n",
    "    for combo in combinations:\n",
    "        new_combo = []\n",
    "        for index in corr:\n",
    "            new_combo.append(combo[index])\n",
    "        new_combinations.append(new_combo)\n",
    "    combinations = new_combinations\n",
    "    \n",
    "    # Multiprocessing for faster evaluation.\n",
    "    pool = multiprocessing.Pool(num_processors)\n",
    "    lst = [(pca.controlset, pca.ambient_hamiltonian, combo, pca.dt,\n",
    "            pca.control_hamiltonians, pca.target_operator, pca.probs)\n",
    "           for combo in combinations]\n",
    "    projs_fidelities = pool.map(compute_dpn_and_fid, lst)\n",
    "    pool.close()\n",
    "\n",
    "    # Compute the off diagonals and fidelities.\n",
    "    projs = [pf[0] for pf in projs_fidelities]\n",
    "    fidelities = [pf[1] for pf in projs_fidelities]\n",
    "    projs = np.vstack(projs).T\n",
    "    fidelities = np.vstack(fidelities).T\n",
    "    plt.figure(figsize=(16, 8))  # the first figure\n",
    "    plt.subplot(211)  # the first subplot in the first figure\n",
    "    indices = list(range(len(values)))\n",
    "    for i, row in enumerate(projs[:-1, :]):\n",
    "        reordered_row = np.array([row[j] for j in indices])\n",
    "        plt.plot(values, reordered_row)\n",
    "    plt.plot(values, [projs[-1, :][i] for i in indices], label=\"min\", color='k', linewidth=2, zorder=10)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Absolute Sum of Off Diagonal Elements\")\n",
    "    plt.semilogy()\n",
    "    plt.subplot(212)  # the second subplot in the first figure\n",
    "    for i, row in enumerate(fidelities[:-1, :]):\n",
    "        reordered_row = np.array([row[j] for j in indices])\n",
    "        plt.plot(values, -np.log(1 - reordered_row))\n",
    "    plt.plot(values, [-np.log(1 - fidelities[-1, :][i]) for i in indices], \n",
    "             label=\"min\", color='k', linewidth=2, zorder=10)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"f\")\n",
    "    samples = np.linspace(plt.ylim()[0], plt.ylim()[1], 11)\n",
    "    labels = -(np.exp(-samples) - 1)\n",
    "    plt.xlabel(\"Detuning\")\n",
    "    plt.tight_layout()\n",
    "    plt.yticks(samples, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of how plots were generated in the old version of the paper - where we sample a grid of detuning values. If you could manage a heat map on a cluster, I think that would look really neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_indices(num_points, order_desired):\n",
    "    num_indices = len(order_desired)\n",
    "    tuples = product(range(num_points), repeat=num_indices)\n",
    "    indices = [sum([num_points**(num_indices - 1 - order_desired[i]) * t[i] \n",
    "                    for i in range(num_indices)]) for t in tuples]\n",
    "    return indices\n",
    "\n",
    "values_to_plot = []\n",
    "corr = []\n",
    "for i, detuning in enumerate(pca.detunings):\n",
    "    # This sets the x axis (It's okay to pick detuning[0] because they were all set to the same detuning.)\n",
    "    values = np.linspace(-detuning[0], detuning[0], num_points)\n",
    "    values_to_plot.append(values)\n",
    "    corr.append(i)\n",
    "# This was when I was evaluating more detuning combinations before, \n",
    "# now this is just indexing over each detuning param.\n",
    "combinations = itertools.product(*values_to_plot)\n",
    "new_combinations = []\n",
    "for combo in combinations:\n",
    "    new_combo = []\n",
    "    for index in corr:\n",
    "        new_combo.append(combo[index])\n",
    "    new_combinations.append(new_combo)\n",
    "combinations = new_combinations\n",
    "\n",
    "tuple_length = len(combinations[0])\n",
    "standard_ordering = list(range(tuple_length))\n",
    "ordering = standard_ordering\n",
    "indices = generate_indices(len(values), ordering)\n",
    "\n",
    "# Multiprocessing for faster evaluation.\n",
    "pool = multiprocessing.Pool(num_processors)\n",
    "lst = [(pca.controlset, pca.ambient_hamiltonian, combo, pca.dt,\n",
    "        pca.control_hamiltonians, pca.target_operator, pca.probs)\n",
    "       for combo in combinations]\n",
    "projs_fidelities = pool.map(compute_dpn_and_fid, lst)\n",
    "pool.close()\n",
    "\n",
    "# Compute the off diagonals and fidelities.\n",
    "projs = [pf[0] for pf in projs_fidelities]\n",
    "fidelities = [pf[1] for pf in projs_fidelities]\n",
    "projs = np.vstack(projs).T\n",
    "fidelities = np.vstack(fidelities).T\n",
    "plt.figure(figsize=(16, 8))  # the first figure\n",
    "plt.subplot(211)  # the first subplot in the first figure\n",
    "\n",
    "for i, row in enumerate(projs[:-1, :]):\n",
    "    reordered_row = np.array([row[j] for j in indices])\n",
    "    plt.plot(range(len(row)), reordered_row)\n",
    "plt.plot(range(len(projs[-1, :])), [projs[-1, :][i] for i in indices], \n",
    "         label=\"min\", color='k', linewidth=2, zorder=10)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Absolute Sum of Off Diagonal Elements\")\n",
    "plt.semilogy()\n",
    "plt.subplot(212)  # the second subplot in the first figure\n",
    "for i, row in enumerate(fidelities[:-1, :]):\n",
    "    reordered_row = np.array([row[j] for j in indices])\n",
    "    plt.plot(range(len(row)), -np.log(1 - reordered_row))\n",
    "plt.plot(range(len(fidelities[-1, :])), [-np.log(1 - fidelities[-1, :][i]) for i in indices], \n",
    "         label=\"min\", color='k', linewidth=2, zorder=10)\n",
    "plt.legend()\n",
    "plt.ylabel(\"f\")\n",
    "samples = np.linspace(plt.ylim()[0], plt.ylim()[1], 11)\n",
    "labels = -(np.exp(-samples) - 1)\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.tight_layout()\n",
    "plt.yticks(samples, labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The order and value of the detunings plotted is given by:\n",
    "print([combinations[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fidelities = np.reshape(fidelities, (fidelities.shape[0], num_points, num_points))\n",
    "sample_points = np.outer(values, values)\n",
    "\n",
    "for fid in fidelities:\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.mlab as mlab\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    X = sorted(set(list(zip(*sample_points))[0]))\n",
    "    Y = sorted(set(list(zip(*sample_points))[1]))\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    fid = fid.reshape(len(X), len(X))\n",
    "    fid = 1 - fid\n",
    "    Z = fid \n",
    "    plt.figure()\n",
    "    CS = plt.contour(X, Y, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.title('Infidelity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diamond Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# from pauli_channel_approximation import control_unitaries, error_unitary, off_diagonal_projection, adjoint\n",
    "import sys\n",
    "import itertools\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "\n",
    "from diamond import diamond_norm, jamiolkowski\n",
    "\n",
    "def superoperator(unitary):\n",
    "\n",
    "    return np.kron(unitary.conj(), unitary)\n",
    "\n",
    "def choi_form(unitary):\n",
    "\n",
    "    return jamiolkowski(superoperator(unitary))\n",
    "\n",
    "def diamond_distance(unitary_a,unitary_b):\n",
    "    return diamond_norm(choi_form(unitary_a) - choi_form(unitary_b))/2.\n",
    "\n",
    "def compute_diamond_norm(data):\n",
    "    controlset, ambient_hamiltonian0, combo, dt, control_hamiltonians, target_operator, probs = data\n",
    "    print(\"DOING COMBO {}\".format(combo))\n",
    "    sys.stdout.flush()\n",
    "    fidelities = []\n",
    "    projs = []\n",
    "    sops = []\n",
    "    controlset_unitaries = []\n",
    "    d_norms = []\n",
    "    #\n",
    "    #\n",
    "    # for i, com in enumerate(combo):\n",
    "    #     if i != 0 and com != 0:\n",
    "    #         return 0\n",
    "    for controls in controlset:\n",
    "        newcontrols = deepcopy(controls)\n",
    "        ambient_hamiltonian = [deepcopy(ah).astype(\"float\") for ah in ambient_hamiltonian0]\n",
    "        for cnum, value in enumerate(combo):\n",
    "            cnum -= len(ambient_hamiltonian0)\n",
    "            if cnum >= 0:\n",
    "                newcontrols[:, cnum] = newcontrols[:, cnum] * (1 + value)\n",
    "            if cnum < 0:\n",
    "                ambient_hamiltonian[cnum] *= float(value)\n",
    "        step_unitaries = control_unitaries(ambient_hamiltonian,\n",
    "                                           control_hamiltonians, newcontrols,\n",
    "                                           dt)\n",
    "        unitary = reduce(lambda a, b: a.dot(b), step_unitaries)\n",
    "        sop = error_unitary(unitary, target_operator)\n",
    "        sops.append(sop)\n",
    "        d_norms.append(diamond_distance(sop, np.eye(sop.shape[0])))\n",
    "    avg_sop = reduce(lambda a, b: a + b, [prob * sops[i] for i, prob in enumerate(probs)])\n",
    "    d_norms.append(diamond_distance(avg_sop, np.eye(avg_sop.shape[0])))\n",
    "\n",
    "    return d_norms\n",
    "\n",
    "def generate_indices(num_points, order_desired):\n",
    "    num_indices = len(order_desired)\n",
    "    tuples = product(range(num_points), repeat=num_indices)\n",
    "    indices = [sum([num_points**(num_indices - 1 - order_desired[i]) * t[i] \n",
    "                    for i in range(num_indices)]) for t in tuples]\n",
    "    return indices\n",
    "\n",
    "values_to_plot = []\n",
    "corr = []\n",
    "for i, detuning in enumerate(pca.detunings):\n",
    "    # This sets the x axis (It's okay to pick detuning[0] because they were all set to the same detuning.)\n",
    "    values = np.linspace(-detuning[0], detuning[0], num_points)\n",
    "    values_to_plot.append(values)\n",
    "    corr.append(i)\n",
    "# This was when I was evaluating more detuning combinations before, now this is just indexing over each detuning param.\n",
    "combinations = itertools.product(*values_to_plot)\n",
    "new_combinations = []\n",
    "for combo in combinations:\n",
    "    new_combo = []\n",
    "    for index in corr:\n",
    "        new_combo.append(combo[index])\n",
    "    new_combinations.append(new_combo)\n",
    "combinations = new_combinations\n",
    "\n",
    "tuple_length = len(combinations[0])\n",
    "standard_ordering = list(range(tuple_length))\n",
    "ordering = standard_ordering\n",
    "indices = generate_indices(len(values), ordering)\n",
    "\n",
    "# Multiprocessing for faster evaluation.\n",
    "pool = multiprocessing.Pool(num_processors)\n",
    "lst = [(pca.controlset, pca.ambient_hamiltonian, combo, pca.dt,\n",
    "        pca.control_hamiltonians, pca.target_operator, pca.probs)\n",
    "       for combo in combinations]\n",
    "d_norms = pool.map(compute_diamond_norm, lst)\n",
    "pool.close()\n",
    "\n",
    "# Compute the off diagonals and fidelities.\n",
    "d_norms = np.vstack(d_norms).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, row in enumerate(d_norms[:-1, :]):\n",
    "    reordered_row = np.array([row[j] for j in indices])\n",
    "    plt.plot(range(len(row)), reordered_row)\n",
    "plt.plot(range(len(d_norms[-1, :])), [d_norms[-1, :][i] for i in indices], label=\"min\", color='k', linewidth=2, zorder=10)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Diamond Distance\")\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_norms = np.reshape(d_norms, (d_norms.shape[0], num_points, num_points))\n",
    "sample_points = np.outer(values, values)\n",
    "\n",
    "for fid in d_norms:\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.mlab as mlab\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    X = sorted(set(list(zip(*sample_points))[0]))\n",
    "    Y = sorted(set(list(zip(*sample_points))[1]))\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    fid = fid.reshape(len(X), len(X))\n",
    "    Z = fid \n",
    "    plt.figure()\n",
    "    CS = plt.contour(X, Y, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.title('Diamond Norm')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
